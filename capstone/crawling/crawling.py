import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import os
import xml.etree.ElementTree as ET
from io import BytesIO
import zipfile

from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException
from tqdm import tqdm
from datetime import datetime

# DART API ê´€ë ¨ ì„¤ì •
DART_API_KEY = "57ffb9ff9bb49909a705b746865ae8dd480ece8a"
bsns_year = "2024"
reprt_code = "11011"
fs_div = "CFS"

# CSV ì½ê¸° (ì—…ì¢… ì¶”ê°€ë¨)
company_df = pd.read_csv("capstone_company_list.csv")
headers = {"User-Agent": "Mozilla/5.0"}
os.makedirs("output", exist_ok=True)

# DART API í˜¸ì¶œ (corpCode.xml)
corp_code_url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={DART_API_KEY}"
res = requests.get(corp_code_url)
if res.status_code == 200:
    with zipfile.ZipFile(BytesIO(res.content)) as z:
        z.extractall("corp_data")
        print("âœ… corp_code.xml ì¶”ì¶œ ì™„ë£Œ")
else:
    print("âŒ corp_code.xml ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
    exit()

corp_tree = ET.parse("corp_data/CORPCODE.xml")
corp_root = corp_tree.getroot()

def get_corp_code(name):
    for item in corp_root.findall("list"):
        if item.findtext("corp_name") == name:
            return item.findtext("corp_code")
    return None

def fetch_financial_statement(corp_name, corp_code):
    url = "https://opendart.fss.or.kr/api/fnlttSinglAcntAll.json"
    params = {
        "crtfc_key": DART_API_KEY,
        "corp_code": corp_code,
        "bsns_year": bsns_year,
        "reprt_code": reprt_code,
        "fs_div": fs_div,
    }
    try:
        res = requests.get(url, params=params)
        data = res.json()
        if data["status"] == "000":
            df = pd.DataFrame(data["list"])
            df["ê¸°ì—…ëª…"] = corp_name
            return df
        else:
            print(f"âš ï¸ {corp_name}: ì¬ë¬´ì œí‘œ ì—†ìŒ - {data['message']}")
            return None
    except Exception as e:
        print(f"âŒ {corp_name} ì¬ë¬´ì œí‘œ ì˜¤ë¥˜: {e}")
        return None

BASE_URL = "https://finance.naver.com/item"
TAB_MAP = {
    "ì‹œì„¸": "ì‹œì„¸",
    "ì°¨íŠ¸": "ì°¨íŠ¸",
    "íˆ¬ììë§¤ë§¤ë™í–¥": "íˆ¬ììë§¤ë§¤ë™í–¥",
    "ë‰´ìŠ¤ê³µì‹œ": "ë‰´ìŠ¤ê³µì‹œ",
    "ì¢…ëª©ë¶„ì„": "ì¢…ëª©ë¶„ì„",
    "ì¢…ëª©í† ë¡ ì‹¤": "ì¢…ëª©í† ë¡ ì‹¤",
    "ê³µë§¤ë„í˜„í™©": "ê³µë§¤ë„í˜„í™©"
}

def get_html(url, headers):
    res = requests.get(url, headers=headers)
    return res.text if res.status_code == 200 else None

def crawl_table_from_url(url, headers):
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.select_one("table.type2")
    data = []
    if table:
        for tr in table.select("tr")[2:]:
            tds = tr.find_all("td")
            if len(tds) == 7:
                row = {
                    "ë‚ ì§œ": tds[0].get_text(strip=True),
                    "ì¢…ê°€": tds[1].get_text(strip=True),
                    "ì „ì¼ë¹„": tds[2].get_text(strip=True),
                    "ì‹œê°€": tds[3].get_text(strip=True),
                    "ê³ ê°€": tds[4].get_text(strip=True),
                    "ì €ê°€": tds[5].get_text(strip=True),
                    "ê±°ë˜ëŸ‰": tds[6].get_text(strip=True)
                }
                data.append(row)
    return data

def crawl_chart_image(url, code):
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)

    try:
        # ë¶ˆí•„ìš”í•œ ë°°ë„ˆë‚˜ ê´‘ê³  ìš”ì†Œ ìˆ¨ê¸°ê¸°
        driver.execute_script("""
            var ads = document.querySelectorAll('.ad-banner, .popup');
            ads.forEach(function(ad) {
                ad.style.display = 'none';
            });
        """)

        # ì°¨íŠ¸ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        chart_element = WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, '.ciq-chart-area'))
        )

        # ì°¨íŠ¸ ìº¡ì³
        chart_element.screenshot(f"./output/{today_str}_{code}_chart_image.png")
    except TimeoutException:
        print(f"âš ï¸ {code}: ì°¨íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¸ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ {code}: ì°¨íŠ¸ ìŠ¤í¬ë¦°ìƒ· ì‹¤íŒ¨ - {e}")
    finally:
        driver.quit()

def crawl_frgn_trading(url, headers):
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.select_one("table.type2")
    data = []
    if table:
        for tr in table.select("tr"):
            tds = tr.select("td")
            if len(tds) == 4:
                data.append({
                    "ë§¤ë„ìƒìœ„": tds[0].get_text(strip=True),
                    "ë§¤ë„ê±°ë˜ëŸ‰": tds[1].get_text(strip=True),
                    "ë§¤ìˆ˜ìƒìœ„": tds[2].get_text(strip=True),
                    "ë§¤ìˆ˜ê±°ë˜ëŸ‰": tds[3].get_text(strip=True)
                })
        total = table.select_one("tr.total")
        if total:
            tds = total.select("td")
            data.append({
                "ì™¸êµ­ê³„ì¶”ì •í•©_ë§¤ë„": tds[1].get_text(strip=True),
                "ì™¸êµ­ê³„ì¶”ì •í•©_ë§¤ìˆ˜": tds[2].get_text(strip=True),
                "ì™¸êµ­ê³„ì¶”ì •í•©_ê±°ë˜ëŸ‰": tds[3].get_text(strip=True)
            })
    return data

def crawl_short_trade(url):
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    tbody = soup.find('tbody', class_='CI-GRID-BODY-TABLE-TBODY')
    rows = tbody.find_all('tr')
    data = []
    for row in rows:
        cols = row.find_all('td')
        if len(cols) == 9:
            data.append({
                'ì¼ì': cols[0].text.strip(),
                'ê±°ë˜ëŸ‰ ì „ì²´': cols[1].text.strip(),
                'ê±°ë˜ëŸ‰ ì—…í‹±ë¥ ì ìš©': cols[2].text.strip(),
                'ê±°ë˜ëŸ‰ ì—…í‹±ë¥ ì˜ˆì™¸': cols[3].text.strip(),
                'ìˆœë³´ìœ  ì”ê³ ìˆ˜ëŸ‰': cols[4].text.strip(),
                'ê±°ë˜ëŒ€ê¸ˆ ì „ì²´': cols[5].text.strip(),
                'ê±°ë˜ëŒ€ê¸ˆ ì—…í‹±ë¥ ì ìš©': cols[6].text.strip(),
                'ê±°ë˜ëŒ€ê¸ˆ ì—…í‹±ë¥ ì˜ˆì™¸': cols[7].text.strip(),
                'ìˆœë³´ìœ  ì”ê³ ê¸ˆì•¡': cols[8].text.strip()
            })
    driver.quit()
    return pd.DataFrame(data)

def crawl_google_news(company_name):
    from selenium.common.exceptions import NoSuchElementException, WebDriverException

    news_data = []

    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    query = company_name
    url = f"https://www.google.com/search?q={query}&tbm=nws&hl=ko&gl=kr"
    driver.get(url)
    time.sleep(3)

    articles = driver.find_elements(By.CSS_SELECTOR, "#rso > div > div > div")

    for idx, article in enumerate(articles[:10], 1):
        try:
            title = article.find_element(By.CSS_SELECTOR, "div.n0jPhd.ynAwRc.MBeuO.nDgy9d").text.strip()
            press = article.find_element(By.CSS_SELECTOR, "div.MgUUmf span").text.strip()
            date = article.find_element(By.CSS_SELECTOR, "div.OSrXXb span").text.strip()
            link = article.find_element(By.CSS_SELECTOR, "a").get_attribute("href")

            driver.execute_script("window.open(arguments[0]);", link)
            driver.switch_to.window(driver.window_handles[1])
            time.sleep(4)

            try:
                content = driver.find_element(By.TAG_NAME, "body").get_attribute("innerText").strip()
            except:
                content = "ë³¸ë¬¸ ì—†ìŒ"

            # ì°½ì„ ë‹«ê¸° ì „ì— ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸
            if len(driver.window_handles) > 1:
                driver.close()
                driver.switch_to.window(driver.window_handles[0])

            if len(content) >= 500:
                content_trimmed = content[50:-50]  # ì•ë’¤ 50ìì”© ì œê±°í•˜ê³  ì¤‘ê°„ë§Œ ë‚¨ê¹€
            else:
                content_trimmed = content

            news_data.append({
                "ì œëª©": title,
                "ì–¸ë¡ ì‚¬": press,
                "ë‚ ì§œ": date,
                "ë³¸ë¬¸": content_trimmed,
                "ë§í¬": link
            })

        except (NoSuchElementException, WebDriverException) as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e} (ê¸°ì‚¬: {idx})")
            continue  # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°

    driver.quit()
    return news_data


def crawl_company_info(company_name):
    # ì›¹ë“œë¼ì´ë²„ ì„¤ì •
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    # ë„¤ì´ë²„ ì¦ê¶Œ ê¸°ì—…ê°œìš” í˜ì´ì§€ë¡œ ì´ë™
    base_url = f"https://navercomp.wisereport.co.kr/v2/company/c1020001.aspx?cmp_cd=318160&cn="
    driver.get(base_url)
    time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    
    try:
        # ë³¸ì‚¬ì£¼ì†Œ, í™ˆí˜ì´ì§€, ëŒ€í‘œì´ì‚¬, ì„¤ë¦½ì¼ í¬ë¡¤ë§
        address = driver.find_element(By.XPATH, '//*[@id="cTB201"]/tbody/tr[1]/td').text.strip()  # ë³¸ì‚¬ì£¼ì†Œ
        homepage = driver.find_element(By.XPATH, '//*[@id="cTB201"]/tbody/tr[2]/td[1]/a').get_attribute("href").strip()  # í™ˆí˜ì´ì§€
        ceo = driver.find_element(By.XPATH, '//*[@id="cTB201"]/tbody/tr[3]/td[2]').text.strip()  # ëŒ€í‘œì´ì‚¬
        establishment_date = driver.find_element(By.XPATH, '//*[@id="cTB201"]/tbody/tr[3]/td[1]').text.strip()  # ì„¤ë¦½ì¼

        # í¬ë¡¤ë§í•œ ì •ë³´ ì €ì¥
        company_info = {
            "ë³¸ì‚¬ì£¼ì†Œ": address,
            "í™ˆí˜ì´ì§€": homepage,
            "ëŒ€í‘œì´ì‚¬": ceo,
            "ì„¤ë¦½ì¼": establishment_date
        }

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        company_info = {}

    driver.quit()
    return company_info


today_str = datetime.today().strftime('%Y%m%d')

def crawl_tab_content(url, code, section, sector=None):
    if section == "ì‹œì„¸":
        main_url = f"{BASE_URL}/main.naver?code={code}"
        time_url = f"{BASE_URL}/sise_time.naver?code={code}"
        day_url = f"{BASE_URL}/sise_day.naver?code={code}"
        return {
            "ì£¼ìš”ì‹œì„¸": get_html(main_url, headers),
            "ì‹œê°„ë³„ì‹œì„¸": get_html(time_url, headers),
            "ì¼ë³„ì‹œì„¸": crawl_table_from_url(day_url, headers)
        }
    elif section == "ì°¨íŠ¸":
        crawl_chart_image(f"{BASE_URL}/fchart.naver?code={code}", code)
        return {"ì°¨íŠ¸": "ì €ì¥ë¨"}
    elif section == "íˆ¬ììë§¤ë§¤ë™í–¥":
        return crawl_frgn_trading(f"{BASE_URL}/frgn.naver?code={code}", headers)
    elif section == "ë‰´ìŠ¤ê³µì‹œ":
        return crawl_google_news(name)  # 'name' â†’ 'code' ë˜ëŠ” ì‹¤ì œ íšŒì‚¬ëª…
    elif section == "ì¢…ëª©ë¶„ì„":
        # ì—…ì¢… ì¶”ê°€
        company_info = crawl_company_info(name)  # í¬ë¡¤ë§í•œ ê¸°ì—…ì •ë³´ ì¶”ê°€
        return {
            "ì—…ì¢…": sector,
            "ë³¸ì‚¬ì£¼ì†Œ": company_info.get("ë³¸ì‚¬ì£¼ì†Œ"),
            "í™ˆí˜ì´ì§€": company_info.get("í™ˆí˜ì´ì§€"),
            "ëŒ€í‘œì´ì‚¬": company_info.get("ëŒ€í‘œì´ì‚¬"),
            "ì„¤ë¦½ì¼": company_info.get("ì„¤ë¦½ì¼"),
            "ì¢…ëª©ë¶„ì„": crawl_table_from_url(f"{BASE_URL}/coinfo.naver?code={code}", headers)
        }
    elif section == "ì¢…ëª©í† ë¡ ì‹¤":
        soup = BeautifulSoup(get_html(f"{BASE_URL}/board.nhn?code={code}", headers), 'html.parser')
        posts = []
        for tr in soup.select('table.type2 tbody tr')[5:]:
            title_tag = tr.select_one('td.title a')
            date_tag = tr.select_one('td span')
            if title_tag:
                link = "https://finance.naver.com" + title_tag['href']
                content = BeautifulSoup(requests.get(link, headers=headers).text, 'html.parser').select_one('#body')
                posts.append({
                    "title": title_tag.text.strip(),
                    "date": date_tag.text.strip() if date_tag else "ë‚ ì§œ ì—†ìŒ",
                    "content": content.text.strip() if content else "ë‚´ìš© ì—†ìŒ"
                })
        return posts
    elif section == "ê³µë§¤ë„í˜„í™©":
        return crawl_short_trade(f"https://data.krx.co.kr/comm/srt/srtLoader/index.cmd?screenId=MDCSTAT300&isuCd={code}")


# ê¸°ì—… í¬ë¡¤ë§ ì‹¤í–‰
for idx, row in tqdm(company_df.iterrows(), total=len(company_df), desc="ê¸°ì—… í¬ë¡¤ë§ ì§„í–‰"):
    name = row['ê¸°ì—…ëª…']
    code = str(row['ì¢…ëª©ì½”ë“œ']).zfill(6)
    sector = row['ì—…ì¢…']  # ì—…ì¢… ë°ì´í„° ì¶”ê°€

    print(f"\nğŸ“¦ {name} ({code}) í¬ë¡¤ë§ ì¤‘...")

    result = {"ê¸°ì—…ëª…": name, "ì¢…ëª©ì½”ë“œ": code, "ì—…ì¢…": sector}  # ì—…ì¢… í¬í•¨

    # ë³¸ì‚¬ì£¼ì†Œ, í™ˆí˜ì´ì§€, ëŒ€í‘œì´ì‚¬, ì„¤ë¦½ì¼ í¬ë¡¤ë§
    company_info = crawl_company_info(name)
    result.update(company_info)  # ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¶”ê°€

    df_short = crawl_short_trade(f"https://data.krx.co.kr/comm/srt/srtLoader/index.cmd?screenId=MDCSTAT300&isuCd={code}")
    result["ê³µë§¤ë„í˜„í™©"] = df_short.to_dict(orient="records")

    for section, title in TAB_MAP.items():
        if section == "ê³µë§¤ë„í˜„í™©":
            continue
        print(f" â†’ {title} í¬ë¡¤ë§ ì¤‘...")
        result[title] = crawl_tab_content(f"{BASE_URL}/{section}.naver?code={code}", code, title, sector)
        time.sleep(10)

    corp_code = get_corp_code(name)
    if corp_code:
        df_fin = fetch_financial_statement(name, corp_code)
        if df_fin is not None:
            df_fin.to_csv(f"output/{today_str}_{name}_{bsns_year}_ì¬ë¬´ì œí‘œ.csv", index=False, encoding="utf-8-sig")

    with open(f"output/{today_str}_{name}.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    df_short.to_csv(f"output/{today_str}_{name}_short_trade.csv", index=False, encoding="utf-8-sig")
    time.sleep(5)
